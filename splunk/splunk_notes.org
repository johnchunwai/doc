* glossary
** indexer cluster
| master node            | manage the cluster (choose master/target nodes,             |
|                        | balance buckets among peers etc)                            |
| peer nodes             | index, replicate, search data                               |
| source node            | peer that ingest data and send replication                  |
| target node            | receive streams of replicated data from source nodes        |
| replication factor     | how many copies of data in cluster in total                 |
| search factor          | how many searchable copies of data in cluster               |
| buckets                | basic units of index storage                                |
| bucket stage           | hot, warm, code, frozen                                     |
| search head            | ask master node and send search requests to peers nodes     |
| complete               | cluster satisfies both search factor and replication factor |
| valid                  | a cluster has exactly one primary copy of each bucket       |
| primary bucket         | searchable bucket that participates in a search             |
| generation             | a snapshot of the set of primary bucket copies              |
| indexer acknowledgment | when forwarder doesn't get ack from peer, it resends        |

* cluster indexing and searching

** clustered indexing
- peer node registers with master
- master rebalances the primary bucket copies across the cluster and
  starts a new generation with a generation id
- generation tracks primary buck copies and on which peers they're located
- master communicates the current gen id to peers and search head
- peers tracks which bucket copies are primary for each gen
- source peer ingests data, stores them into hot bucket
- master gives source peer a list of target peers (same count as
  replication factor if possible), and which peers should be
  searchable
- source streams rawdata in blocks to targets, tells targets if their
  copy should be searchable
- targets with searchable start indexing the files
- when source rolls a hot bucket, it tells master and targets

** cluster searching
- search heads poll master for latest gen info at regular intervals
- when gen changes, master give the search heads new gen id and list
  of peers for that gen
- search head gives the peers gen id when it inits a search
- peers use gen id to identify the buckets for the search

** when generation changes
- master comes online
- peer joins cluster
- peer goes down
- when rebalancing of primary copies occurs (eg. rebalance_primaries)

** when bucket changes stages
- from hot to warm to cold, nothing
- from cold to frozen:
  - first peer informs master
  - master alerts other peers
  - when all copies are frozen (delete or archive), master removes the
    bucket from the list

* admin
** help
#+BEGIN_SRC 
splunk help commands
splunk help <command>
#+END_SRC

** settings files
*** single instance
#+BEGIN_SRC 
# input
splunk/etc/.../local/inputs.conf
# index and sourcetype
splunk/etc/apps/<app_name>/local/*
# field extraction
splunk/etc/users/<admin>/<app_name>/local/props.conf
#+END_SRC

*** multi instance

**** forwarder
#+BEGIN_SRC 
# tradehouse/local/inputs.conf
[monitor blah]
sourcetype = blah
index = blah

# system/local/inputs.conf
[default]
host = forwarder host

# system/local/outputs.conf
[indexAndForward]
index = false
[tcpout]
defaultGroup = lion-indexers
forwardedindex.filter.disable = true
indexAndForward = false
[tcpout:lion-indexers]
server = <list of host for splunk indexers>
autoLB = true
#+END_SRC

**** indexer
#+BEGIN_SRC 
# inputs.conf
[default]
host = <my host>
[splunktcp://<port where forward send data to>]
disabled = 0
queueSize = 10MB

# indexes.conf
[<index_name>]
<bucket>Path = <path>
maxTotalDataSizeMB = <blah>
maxDataSize = auto_high_volume

# transforms.conf
data filtering
field extractions

# props.conf
[<sourcetype>]
TRANSFORMS-set = <things in transforms.conf>
#+END_SRC


** data
*** add data
#+BEGIN_SRC 
# use web
OR
splunk add monitor /var/log
#+END_SRC
- for multiline data, may need to update
  $SPLUNK_HOME/etc/apps/search/local/props.conf as follows
#+BEGIN_SRC 
[your sourcetype name]
# don't truncate
TRUNCATE = 0
# max lines for an event
MAX_EVENTS = 10000
#+END_SRC
- set min free space
#+BEGIN_SRC 
# do it from Settings -> General
OR
splunk set minfreemb 20000
splunk restart
#+END_SRC
-- make sourcetype searchable without index
#+BEGIN_SRC 
# edit local/authorize.conf
# under [role_<usertype>]:
srchIndexesDefault = main;<other exiting entries>;<+index to be searchable>
#+END_SRC

*** delete data
- stop and restart splunk between these commands
- delete data only
#+BEGIN_SRC 
# delete data from a certain index
splunk clean eventdata -index <index_name>
#+END_SRC
- delete data + index
#+BEGIN_SRC 
splunk remove index <index_name>
# may have to clean up index ref from props.conf
#+END_SRC
- delete sourcetype
#+BEGIN_SRC 
# remove the entries in the local folder that owns the sourcetype
# remove stanza's from (app|owner)/metadata (../metadata from local)
# may have to remove field extraction regex from:
#     splunk/etc/users/<admin>/search/local/props.conf
# may have to clean up index ref from props.conf
#+END_SRC
- delete input
#+BEGIN_SRC 
# remove input.conf
# remove stanza in local/../metadata/local.meta
#+END_SRC

** remove app from splunk
#+BEGIN_SRC 
# figure out the folder name from Apps -> Manage Apps
sudo /opt/splunk/bin/splunk stop
sudo /opt/splunk/bin/splunk remove app <folder name>
OR
sudo rm -rf /opt/splunk/etc/apps/<folder name>
sudo rm -rf /opt/splunk/etc/users/*/<folder name> (if exist)
sudo /opt/splunk/bin/splunk start
#+END_SRC
